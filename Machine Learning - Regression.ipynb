{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Of Contents\n",
    "* [scikit-learn](#scikitlearn)\n",
    "* [Math](#math)\n",
    "* [Linear Models](#linearmodels)\n",
    "    * [Linear Regression](#linearregression)\n",
    "    * [Lasso](#lasso)\n",
    "    * [ElasticNet](#elastic)\n",
    "* [Support Vector Machines](#svm)\n",
    "* [Stochastic Gradient Descent](#sgd)\n",
    "* [K-Nearest Neighbors](#knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn Basics <a class=\"anchor\" id=\"scikitlearn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally not considered good practice to import the entire scikit-learn package as one statement, but rather to import only the packages that are needed at the time. For instance, if you wanted to use `LinearRegression` you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn also includes a number of useful datasets, which can be imported directly. Once the dataset has been loaded, in this case the Boston Housing Prices dataset, we use a `pandas.DataFrame` to hold the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no getting around the fact that machine learning is built heavily upon mathematics. For this series of meetups about machine learning, we decided that \"a light dusting\" would be the right amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow <a class=\"anchor\" id=\"machinelearningworkflow\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we work with datasets in machine learning, we have to partition the data in such a way that we know that the model is actually learning instead of simply memorizing the data that was previously given to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn's train_test_split\n",
    "Scikit-Learn has a built-in method for splitting a dataset into train and test sets and is much better than trying to split a dataset by hand. You can specify the ratio of train/test, along with seeding a random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once loaded, we can create our train/test sets, with the convention that `X` is the training set and `y` is the test set. We will use the `boston.data` for our training set and `boston.target` for our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDRegressor <a class=\"anchor\" id=\"sgd\"/>\n",
    "\n",
    "According to the diagram, Stochastic Gradient Descent works best when we have greater than 100,000 samples. While the Boston Housing dataset does not, we can find a regression dataset that does. For this, we can turn to the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets.html?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=instDown&view=table), in this case the [SGEMM GPU Kernel Performance Dataset](http://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MWG</th>\n",
       "      <th>NWG</th>\n",
       "      <th>KWG</th>\n",
       "      <th>MDIMC</th>\n",
       "      <th>NDIMC</th>\n",
       "      <th>MDIMA</th>\n",
       "      <th>NDIMB</th>\n",
       "      <th>KWI</th>\n",
       "      <th>VWM</th>\n",
       "      <th>VWN</th>\n",
       "      <th>STRM</th>\n",
       "      <th>STRN</th>\n",
       "      <th>SA</th>\n",
       "      <th>SB</th>\n",
       "      <th>Run1 (ms)</th>\n",
       "      <th>Run2 (ms)</th>\n",
       "      <th>Run3 (ms)</th>\n",
       "      <th>Run4 (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "      <td>241600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>80.415364</td>\n",
       "      <td>80.415364</td>\n",
       "      <td>25.513113</td>\n",
       "      <td>13.935894</td>\n",
       "      <td>13.935894</td>\n",
       "      <td>17.371126</td>\n",
       "      <td>17.371126</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.448609</td>\n",
       "      <td>2.448609</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>217.647852</td>\n",
       "      <td>217.579536</td>\n",
       "      <td>217.532756</td>\n",
       "      <td>217.527669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>42.469220</td>\n",
       "      <td>42.469220</td>\n",
       "      <td>7.855619</td>\n",
       "      <td>7.873662</td>\n",
       "      <td>7.873662</td>\n",
       "      <td>9.389418</td>\n",
       "      <td>9.389418</td>\n",
       "      <td>3.000006</td>\n",
       "      <td>1.953759</td>\n",
       "      <td>1.953759</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>369.012422</td>\n",
       "      <td>368.677309</td>\n",
       "      <td>368.655118</td>\n",
       "      <td>368.677413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.290000</td>\n",
       "      <td>13.250000</td>\n",
       "      <td>13.360000</td>\n",
       "      <td>13.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.660000</td>\n",
       "      <td>40.710000</td>\n",
       "      <td>40.660000</td>\n",
       "      <td>40.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>69.825000</td>\n",
       "      <td>69.930000</td>\n",
       "      <td>69.790000</td>\n",
       "      <td>69.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>228.530000</td>\n",
       "      <td>228.310000</td>\n",
       "      <td>228.320000</td>\n",
       "      <td>228.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3339.630000</td>\n",
       "      <td>3375.420000</td>\n",
       "      <td>3397.080000</td>\n",
       "      <td>3361.710000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MWG            NWG            KWG          MDIMC  \\\n",
       "count  241600.000000  241600.000000  241600.000000  241600.000000   \n",
       "mean       80.415364      80.415364      25.513113      13.935894   \n",
       "std        42.469220      42.469220       7.855619       7.873662   \n",
       "min        16.000000      16.000000      16.000000       8.000000   \n",
       "25%        32.000000      32.000000      16.000000       8.000000   \n",
       "50%        64.000000      64.000000      32.000000       8.000000   \n",
       "75%       128.000000     128.000000      32.000000      16.000000   \n",
       "max       128.000000     128.000000      32.000000      32.000000   \n",
       "\n",
       "               NDIMC          MDIMA          NDIMB            KWI  \\\n",
       "count  241600.000000  241600.000000  241600.000000  241600.000000   \n",
       "mean       13.935894      17.371126      17.371126       5.000000   \n",
       "std         7.873662       9.389418       9.389418       3.000006   \n",
       "min         8.000000       8.000000       8.000000       2.000000   \n",
       "25%         8.000000       8.000000       8.000000       2.000000   \n",
       "50%         8.000000      16.000000      16.000000       5.000000   \n",
       "75%        16.000000      32.000000      32.000000       8.000000   \n",
       "max        32.000000      32.000000      32.000000       8.000000   \n",
       "\n",
       "                 VWM            VWN           STRM           STRN  \\\n",
       "count  241600.000000  241600.000000  241600.000000  241600.000000   \n",
       "mean        2.448609       2.448609       0.500000       0.500000   \n",
       "std         1.953759       1.953759       0.500001       0.500001   \n",
       "min         1.000000       1.000000       0.000000       0.000000   \n",
       "25%         1.000000       1.000000       0.000000       0.000000   \n",
       "50%         2.000000       2.000000       0.500000       0.500000   \n",
       "75%         4.000000       4.000000       1.000000       1.000000   \n",
       "max         8.000000       8.000000       1.000000       1.000000   \n",
       "\n",
       "                  SA             SB      Run1 (ms)      Run2 (ms)  \\\n",
       "count  241600.000000  241600.000000  241600.000000  241600.000000   \n",
       "mean        0.500000       0.500000     217.647852     217.579536   \n",
       "std         0.500001       0.500001     369.012422     368.677309   \n",
       "min         0.000000       0.000000      13.290000      13.250000   \n",
       "25%         0.000000       0.000000      40.660000      40.710000   \n",
       "50%         0.500000       0.500000      69.825000      69.930000   \n",
       "75%         1.000000       1.000000     228.530000     228.310000   \n",
       "max         1.000000       1.000000    3339.630000    3375.420000   \n",
       "\n",
       "           Run3 (ms)      Run4 (ms)  \n",
       "count  241600.000000  241600.000000  \n",
       "mean      217.532756     217.527669  \n",
       "std       368.655118     368.677413  \n",
       "min        13.360000      13.370000  \n",
       "25%        40.660000      40.640000  \n",
       "50%        69.790000      69.820000  \n",
       "75%       228.320000     228.320000  \n",
       "max      3397.080000    3361.710000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgemm = pd.read_csv('./data/sgemm_product.csv')\n",
    "sgemm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models <a class=\"anchor\" id=\"linearmodels\"/>\n",
    "In regression, the goal is to predict a value for a given set of data. The equation that linear models try to solve for is:\n",
    "\n",
    "$$\\hat{y} = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b$$\n",
    "\n",
    "Here, `x[0]` to `x[p]` denotes the features and `w` and `b` are the parameters of the models that are learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "The simplest form of Linear Regression is known as Ordinary Least Squares (OLS), which minimizes the mean squared error between predictions and the true regression targets, on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights or coefficients, are stored in the `coef_` attribute, while the intercept or offset is stored in `intercept_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr.coef_: [ -1.19858618e-01   4.44233009e-02   1.18612465e-02   2.51295058e+00\n",
      "  -1.62710374e+01   3.84909910e+00  -9.85471557e-03  -1.50002715e+00\n",
      "   2.41507916e-01  -1.10671867e-02  -1.01897720e+00   6.95273216e-03\n",
      "  -4.88110587e-01]\n",
      "lr.intercept_: 37.99259277034278\n"
     ]
    }
   ],
   "source": [
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have fitted our model, we can get to the good stuff, finding out how well our model actually performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.76\n",
      "Test set score: 0.67\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "The primary difference between OLS and ridge lies in the addition of a constrain; we want the magnitude of the coefficients to be as small as possible. This means that each feature should have a very small effect on the outcome, while still having good overall model performance. This constraint is known as regularization, which restricts a model to avoid overfitting. Ridge uses `L2` regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.76\n",
      "Test set score: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adjust the parameter of ridge, known as `alpha` if we think it will give us better performance. By default, `alpha` is set to `1`. Adjusting down may give us better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.76\n",
      "Test set score: 0.67\n"
     ]
    }
   ],
   "source": [
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso <a class=\"anchor\" id=\"lasso\"/>\n",
    "Lasso is similar to Ridge in that it uses regularization, but instead of `L2` regularization Lasso uses `L1` regularization, which zeros some coeffecients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.71\n",
      "Test set score: 0.61\n",
      "Number of features used: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet\n",
    "ElasticNet uses both `L1` and `L2` regularization, with the `l1_ratio` hyperparameter controlling the ratio between `L1` and `L2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.71\n",
      "Test set score: 0.62\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic = ElasticNet().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(elastic.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(elastic.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines <a class=\"anchor\" id=\"svm\"/>\n",
    "Support Vector Machines are another option for solving regression problems. The math behind them is complex, but the use of different kernels can allow them to work on many different types of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.74\n",
      "Test set score: 0.62\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "reg = svm.SVR(kernel='linear')\n",
    "reg.fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(reg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors <a class=\"anchor\" id=\"knn\"/>\n",
    "Regression based on k-nearest neighbors.\n",
    "\n",
    "The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.69\n",
      "Test set score: 0.52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(knn.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.97\n",
      "Test set score: 0.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(rfr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(rfr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
